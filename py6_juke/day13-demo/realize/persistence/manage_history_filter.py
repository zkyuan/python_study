# å¯¼å…¥å¿…è¦çš„ç±»å‹å’Œæ¨¡å—
from typing import Literal

from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState, StateGraph, START
from langgraph.prebuilt import ToolNode

# åˆå§‹åŒ–å†…å­˜ä¿å­˜å™¨
memory = MemorySaver()


# å®šä¹‰ä¸€ä¸ªå·¥å…·å‡½æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿç½‘ç»œæœç´¢
@tool
def search(query: str):
    """Call to surf the web."""
    # è¿™æ˜¯å®é™…å®ç°çš„å ä½ç¬¦
    # ä½†ä¸è¦è®©LLMçŸ¥é“ ğŸ˜Š
    return [
        "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
    ]


# å®šä¹‰å·¥å…·åˆ—è¡¨
tools = [search]
# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
tool_node = ToolNode(tools)
# åˆå§‹åŒ–æ¨¡å‹
model = ChatOpenAI(model_name="gpt-4")
# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
bound_model = model.bind_tools(tools)


# å®šä¹‰å‡½æ•°ï¼Œå†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œ
def should_continue(state: MessagesState) -> Literal["action", "__end__"]:
    """Return the next node to execute."""
    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
    last_message = state["messages"][-1]
    # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™ç»“æŸ
    if not last_message.tool_calls:
        return "__end__"
    # å¦åˆ™ç»§ç»­æ‰§è¡Œ
    return "action"


# å®šä¹‰æ¶ˆæ¯è¿‡æ»¤å‡½æ•°ï¼Œåªä¿ç•™æœ€åä¸€æ¡æ¶ˆæ¯
def filter_messages(messages: list):
    return messages[-1:]


# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„å‡½æ•°
def call_model(state: MessagesState):
    messages = filter_messages(state["messages"])
    response = bound_model.invoke(messages)
    # è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºè¿™ä¼šè¢«æ·»åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­
    return {"messages": response}


# å®šä¹‰ä¸€ä¸ªæ–°çš„çŠ¶æ€å›¾
workflow = StateGraph(MessagesState)

# å®šä¹‰ä¸¤ä¸ªèŠ‚ç‚¹ï¼šagent å’Œ action
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

# è®¾ç½®å…¥å£ç‚¹ä¸º `agent`
workflow.add_edge(START, "agent")

# æ·»åŠ æ¡ä»¶è¾¹ï¼Œæ ¹æ® should_continue å‡½æ•°å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹
workflow.add_conditional_edges(
    "agent",
    should_continue,
)

# æ·»åŠ æ™®é€šè¾¹ï¼Œä» `action` åˆ° `agent`
workflow.add_edge("action", "agent")

# ç¼–è¯‘çŠ¶æ€å›¾ï¼Œå¾—åˆ°ä¸€ä¸ª LangChain Runnable
app = workflow.compile(checkpointer=memory)

# å¯¼å…¥ HumanMessage ç±»
from langchain_core.messages import HumanMessage

# é…ç½®å‚æ•°
config = {"configurable": {"thread_id": "2"}}
# åˆ›å»ºè¾“å…¥æ¶ˆæ¯
input_message = HumanMessage(content="hi! I'm bob")
# é€šè¿‡æµæ¨¡å¼æ‰§è¡Œåº”ç”¨ç¨‹åº
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# ç°åœ¨ä¸ä¼šè®°ä½ä¹‹å‰çš„æ¶ˆæ¯ï¼ˆå› ä¸ºæˆ‘ä»¬åœ¨ filter_messages ä¸­è®¾ç½®äº† `messages[-1:]`ï¼‰
input_message = HumanMessage(content="what's my name?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
