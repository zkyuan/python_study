from typing import Literal

# ä» langchain_openai å¯¼å…¥ ChatOpenAI ç±»ï¼Œç”¨äºä¸ OpenAI çš„ GPT-4 æ¨¡å‹äº¤äº’
from langchain_openai import ChatOpenAI
# ä» langchain_core.tools å¯¼å…¥ tool è£…é¥°å™¨ï¼Œç”¨äºå®šä¹‰å·¥å…·å‡½æ•°
from langchain_core.tools import tool

# ä» langgraph.checkpoint.memory å¯¼å…¥ MemorySaver ç±»ï¼Œç”¨äºä¿å­˜å¯¹è¯çŠ¶æ€
from langgraph.checkpoint.memory import MemorySaver
# ä» langgraph.graph å¯¼å…¥ MessagesState, StateGraph, START ç±»ï¼Œç”¨äºå®šä¹‰å¯¹è¯çŠ¶æ€å’Œå·¥ä½œæµå›¾
from langgraph.graph import MessagesState, StateGraph, START
# ä» langgraph.prebuilt å¯¼å…¥ ToolNode ç±»ï¼Œç”¨äºåˆ›å»ºå·¥å…·èŠ‚ç‚¹
from langgraph.prebuilt import ToolNode

# åˆ›å»ºä¸€ä¸ª MemorySaver å®ä¾‹ï¼Œç”¨äºä¿å­˜å¯¹è¯çŠ¶æ€
memory = MemorySaver()


# ä½¿ç”¨ @tool è£…é¥°å™¨å®šä¹‰ä¸€ä¸ªåä¸º search çš„å·¥å…·å‡½æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿç½‘ç»œæœç´¢
@tool
def search(query: str):
    """Call to surf the web."""
    # è¿™æ˜¯å®é™…å®ç°çš„å ä½ç¬¦ï¼Œä¸è¦è®© LLM çŸ¥é“è¿™ä¸€ç‚¹ ğŸ˜Š
    return [
        "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
    ]


# å®šä¹‰å·¥å…·åˆ—è¡¨ï¼ŒåŒ…å« search å·¥å…·
tools = [search]
# åˆ›å»ºä¸€ä¸ª ToolNode å®ä¾‹ï¼Œä¼ å…¥å·¥å…·åˆ—è¡¨
tool_node = ToolNode(tools)
# åˆ›å»ºä¸€ä¸ª ChatOpenAI å®ä¾‹ï¼Œä½¿ç”¨ GPT-4 æ¨¡å‹
model = ChatOpenAI(model_name="gpt-4")
# å°†å·¥å…·ç»‘å®šåˆ°æ¨¡å‹ä¸Šï¼Œåˆ›å»ºä¸€ä¸ªç»‘å®šäº†å·¥å…·çš„æ¨¡å‹å®ä¾‹
bound_model = model.bind_tools(tools)


# å®šä¹‰ä¸€ä¸ªå‡½æ•° should_continueï¼Œç”¨äºå†³å®šä¸‹ä¸€æ­¥æ˜¯æ‰§è¡ŒåŠ¨ä½œè¿˜æ˜¯ç»“æŸå¯¹è¯
def should_continue(state: MessagesState) -> Literal["action", "__end__"]:
    """Return the next node to execute."""
    # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
    last_message = state["messages"][-1]
    # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œåˆ™ç»“æŸå¯¹è¯
    if not last_message.tool_calls:
        return "__end__"
    # å¦åˆ™ç»§ç»­æ‰§è¡ŒåŠ¨ä½œ
    return "action"


# å®šä¹‰ä¸€ä¸ªå‡½æ•° call_modelï¼Œç”¨äºè°ƒç”¨ç»‘å®šäº†å·¥å…·çš„æ¨¡å‹
def call_model(state: MessagesState):
    # è°ƒç”¨æ¨¡å‹å¹¶è·å–å“åº”
    response = bound_model.invoke(state["messages"])
    # è¿”å›ä¸€ä¸ªåŒ…å«å“åº”æ¶ˆæ¯çš„åˆ—è¡¨
    return {"messages": response}


# åˆ›å»ºä¸€ä¸ªæ–°çš„çŠ¶æ€å›¾ workflowï¼Œä¼ å…¥ MessagesState ç±»
workflow = StateGraph(MessagesState)

# æ·»åŠ åä¸º "agent" çš„èŠ‚ç‚¹ï¼Œå¹¶å°† call_model å‡½æ•°ä¸ä¹‹å…³è”
workflow.add_node("agent", call_model)
# æ·»åŠ åä¸º "action" çš„èŠ‚ç‚¹ï¼Œå¹¶å°† tool_node ä¸ä¹‹å…³è”
workflow.add_node("action", tool_node)

# è®¾ç½®å…¥å£èŠ‚ç‚¹ä¸º "agent"ï¼Œå³ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„èŠ‚ç‚¹
workflow.add_edge(START, "agent")

# æ·»åŠ æ¡ä»¶è¾¹ï¼Œä» "agent" èŠ‚ç‚¹å¼€å§‹ï¼Œä½¿ç”¨ should_continue å‡½æ•°å†³å®šä¸‹ä¸€æ­¥
workflow.add_conditional_edges(
    "agent",
    should_continue,
)

# æ·»åŠ æ™®é€šè¾¹ï¼Œä» "action" èŠ‚ç‚¹åˆ° "agent" èŠ‚ç‚¹ï¼Œå³åœ¨è°ƒç”¨å·¥å…·åè°ƒç”¨æ¨¡å‹
workflow.add_edge("action", "agent")

# ç¼–è¯‘å·¥ä½œæµï¼Œå°†å…¶ç¼–è¯‘æˆ LangChain Runnableï¼Œå¯ä»¥åƒå…¶ä»– runnable ä¸€æ ·ä½¿ç”¨
app = workflow.compile(checkpointer=memory)

# ä» langchain_core.messages å¯¼å…¥ HumanMessage ç±»ï¼Œç”¨äºåˆ›å»ºäººç±»æ¶ˆæ¯
from langchain_core.messages import HumanMessage

# å®šä¹‰é…ç½®å­—å…¸ï¼ŒåŒ…å«å¯é…ç½®é¡¹ "thread_id"
config = {"configurable": {"thread_id": "2"}}
# åˆ›å»ºä¸€ä¸ªäººç±»æ¶ˆæ¯å®ä¾‹ï¼Œå†…å®¹ä¸º "hi! I'm bob"
input_message = HumanMessage(content="hi! I'm bob")
# ä½¿ç”¨ app çš„ stream æ–¹æ³•ï¼Œä¼ å…¥æ¶ˆæ¯å’Œé…ç½®ï¼Œä»¥æµæ¨¡å¼å¤„ç†æ¶ˆæ¯
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    # æ‰“å°æœ€åä¸€æ¡æ¶ˆæ¯
    event["messages"][-1].pretty_print()

# åˆ›å»ºå¦ä¸€ä¸ªäººç±»æ¶ˆæ¯å®ä¾‹ï¼Œå†…å®¹ä¸º "what's my name?"
input_message = HumanMessage(content="what's my name?")
# å†æ¬¡ä½¿ç”¨ app çš„ stream æ–¹æ³•ï¼Œä¼ å…¥æ¶ˆæ¯å’Œé…ç½®ï¼Œä»¥æµæ¨¡å¼å¤„ç†æ¶ˆæ¯
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    # æ‰“å°æœ€åä¸€æ¡æ¶ˆæ¯
    event["messages"][-1].pretty_print()
